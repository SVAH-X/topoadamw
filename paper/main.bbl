\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adams et~al.(2017)Adams, Emerson, Kirby, Neville, Peterson, Shipman,
  Chepushtanova, Hanson, Motta, and Ziegelmeier]{adams2017persistence}
H.~Adams, T.~Emerson, M.~Kirby, R.~Neville, C.~Peterson, P.~Shipman,
  S.~Chepushtanova, E.~Hanson, F.~Motta, and L.~Ziegelmeier.
\newblock Persistence images: A stable vector representation of persistent
  homology.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0 1--35,
  2017.

\bibitem[Carlsson(2009)]{carlsson2009topology}
G.~Carlsson.
\newblock Topology and data.
\newblock \emph{Bulletin of the American Mathematical Society}, 46\penalty0
  (2):\penalty0 255--308, 2009.

\bibitem[Chen et~al.(2019)Chen, Ni, Bai, and Wang]{chen2019topological}
C.~Chen, X.~Ni, Q.~Bai, and Y.~Wang.
\newblock A topological regularizer for classifiers via persistent homology.
\newblock In \emph{Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics}, 2019.

\bibitem[Cohen-Steiner et~al.(2007)Cohen-Steiner, Edelsbrunner, and
  Harer]{cohen2007stability}
D.~Cohen-Steiner, H.~Edelsbrunner, and J.~Harer.
\newblock Stability of persistence diagrams.
\newblock In \emph{Discrete \& Computational Geometry}, volume~37, pages
  103--120, 2007.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Edelsbrunner and Harer(2010)]{edelsbrunner2010computational}
H.~Edelsbrunner and J.~Harer.
\newblock \emph{Computational Topology: An Introduction}.
\newblock American Mathematical Society, 2010.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpness}
P.~Foret, A.~Kleiner, H.~Mobahi, and B.~Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Vinyals, and
  Saxe]{goodfellow2015qualitatively}
I.~J. Goodfellow, O.~Vinyals, and A.~M. Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock \emph{arXiv preprint arXiv:1412.6544}, 2014.

\bibitem[{GUDHI Project}(2021)]{gudhi}
{GUDHI Project}.
\newblock {GUDHI} library.
\newblock \url{https://gudhi.inria.fr}, 2021.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997flat}
S.~Hochreiter and J.~Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
N.~S. Keskar, D.~Mudigere, J.~Nocedal, M.~Smelyanskiy, and P.~T.~P. Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
H.~Li, Z.~Xu, G.~Taylor, C.~Studer, and T.~Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Loshchilov and Hutter(2017{\natexlab{a}})]{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations},
  2017{\natexlab{a}}.

\bibitem[Loshchilov and Hutter(2017{\natexlab{b}})]{loshchilov2017adamw}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017{\natexlab{b}}.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2018convergence}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Rieck et~al.(2019)Rieck, Togninalli, Bock, Moor, Horn, Gumbsch, and
  Borgwardt]{rieck2019neural}
B.~Rieck, M.~Togninalli, C.~Bock, M.~Moor, M.~Horn, T.~Gumbsch, and
  K.~Borgwardt.
\newblock Neural persistence: A complexity measure for deep neural networks
  using algebraic topology.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Smith(2015)]{smith2017cyclical}
L.~N. Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock \emph{arXiv preprint arXiv:1506.01186}, 2015.

\end{thebibliography}
